{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Date:  April 14, 2021\n",
    "#Name:  Peida Han\n",
    "#ID:    2449620842\n",
    "#Email: peidahan@usc.edu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST_data(train_batch_size,test_batch_size):\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081))])\n",
    "    \n",
    "    train_set=tv.datasets.MNIST(\n",
    "        root='./data/',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    train_loader=torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_set=tv.datasets.MNIST(\n",
    "        root='./data/',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_loader=torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    print('Data loaded')\n",
    "    return train_loader,test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=10\n",
    "test_batch_size=1000\n",
    "trainloader,testloader=load_MNIST_data(train_batch_size,test_batch_size)\n",
    "\n",
    "trainData=[]\n",
    "for index,data in enumerate(trainloader):\n",
    "    label=data[1]    \n",
    "    data1=torch.negative(data[0])\n",
    "    trainData.append((data1,label))\n",
    "    trainData.append((data[0],label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=1\n",
    "test_batch_size=1000\n",
    "trainloader,testloader=load_MNIST_data(train_batch_size,test_batch_size)\n",
    "\n",
    "testData=[]\n",
    "posData=[]\n",
    "negData=[]\n",
    "for index,data in enumerate(testloader):\n",
    "    label=data[1]\n",
    "    testData.append([data[0],label]) \n",
    "    data1=torch.negative(data[0])    \n",
    "    testData.append([data1,label])\n",
    "    posData.append(data[0])\n",
    "    negData.append(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of original test data: 0.005889658\n",
      "std of original test data: 1.0077257\n",
      "max of original test data: 2.8214867\n",
      "min of original test data: -0.42421296\n"
     ]
    }
   ],
   "source": [
    "testPos=torch.cat(posData)\n",
    "t=testPos.view(-1,1)\n",
    "print('mean of original test data:', t.mean().numpy())\n",
    "print('std of original test data:', t.std().numpy())\n",
    "print('max of original test data:', t.max().numpy())\n",
    "print('min of original test data:', t.min().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of reversed test data: -0.005889658\n",
      "std of reversed test data: 1.0077257\n",
      "max of reversed test data: 0.42421296\n",
      "min of reversed test data: -2.8214867\n"
     ]
    }
   ],
   "source": [
    "testPos=torch.cat(negData)\n",
    "t=testPos.view(-1,1)\n",
    "print('mean of reversed test data:', t.mean().numpy())\n",
    "print('std of reversed test data:', t.std().numpy())\n",
    "print('max of reversed test data:', t.max().numpy())\n",
    "print('min of reversed test data:', t.min().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 12000\n",
      "each train data size: torch.Size([10, 1, 28, 28])\n",
      "test data len: 20\n",
      "each test data size: torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print('train data len:',len(trainData))\n",
    "print('each train data size:',trainData[0][0].size())\n",
    "print('test data len:',len(testData))\n",
    "print('each test data size:',testData[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self,channel,size):\n",
    "        super(CNN, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.size=size\n",
    "        self.conv1 = nn.Conv2d(channel, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        if self.size==28:\n",
    "            x=F.pad(x,(2,2,2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "def train(cnn,optimizer,criterion,trainloader,testloader,epoch,device):\n",
    "\n",
    "    running_loss=0.0\n",
    "    correct=0\n",
    "    total=0\n",
    "    iter=0\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        inputs,labels=data\n",
    "\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=cnn(inputs)\n",
    "\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        \n",
    "        corr = (predicted == labels)\n",
    "        for j in range(labels.size(0)):\n",
    "            if corr[j].item():\n",
    "                correct+=1\n",
    "            \n",
    "        running_loss+=loss.item()\n",
    "        iter=iter+1\n",
    "        if iter%2000==1999:\n",
    "            print('[%d, %5d] loss:%.5f'%(epoch+1,iter+1,running_loss/2000))\n",
    "            running_loss=0\n",
    "        total+=labels.size(0)\n",
    "    train_acc=  correct/total     \n",
    "    test_acc=predict(device,cnn,testloader)\n",
    "\n",
    "    return train_acc,test_acc  \n",
    "\n",
    "def predict(device,cnn,testloader):\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "                inputs,labels=data\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                labels=labels.float()\n",
    "\n",
    "                outputs=cnn(inputs)\n",
    "                predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "                corr = (predicted == labels)\n",
    "                for i in range(labels.size(0)):\n",
    "                    if corr[i].item():\n",
    "                        correct=correct+1\n",
    "                total+=labels.size(0)\n",
    "\n",
    "    return correct/total       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(device,trainloader,testloader,channel,size,epochs,learning_rate,momentum,weight_decay):\n",
    "    cnn=CNN(channel,size)\n",
    "    cnn=cnn.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(cnn.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "\n",
    "    for epoch in range(epochs):        \n",
    "        train_a,test_a=train(cnn,optimizer,criterion,trainloader,testloader,epoch,device)   \n",
    "        train_acc.append(train_a*100)\n",
    "        test_acc.append(test_a*100)\n",
    "        print('epoch:',epoch+1,'train accuary:',train_a,' test accuary:',test_a)\n",
    "    print('-----Training ends-----')\n",
    "    plt.plot(train_acc,label='train acc')\n",
    "    plt.plot(test_acc,label='test acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.title('Performance Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return train_acc,test_acc\n",
    "\n",
    "def meanTrainResults(device,trainloader,testloader,channel,size,epoch,learning_rate,momentum,weight_decay):\n",
    "    multiple_train_acc=[]\n",
    "    multiple_test_acc=[]\n",
    "\n",
    " \n",
    "    start_time=time.time()\n",
    "    train_acc,test_acc=main(device,trainloader,testloader,channel,size,epoch,learning_rate,momentum,weight_decay)\n",
    "    multiple_train_acc.append(train_acc[-1])    \n",
    "    multiple_test_acc.append(test_acc[-1])\n",
    "\n",
    "    end_time=(time.time()-start_time)/60\n",
    "    print('round:1 total time used:',end_time)\n",
    "\n",
    "    best_acc_train=np.max(multiple_train_acc)\n",
    "    print('Best train accuracy among 5 runs:',best_acc_train)\n",
    "    best_acc_test=np.max(multiple_test_acc)\n",
    "    print('Best test accuracy among 5 runs:',best_acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss:1.02546\n",
      "[1,  4000] loss:0.16758\n",
      "[1,  6000] loss:0.12102\n",
      "[1,  8000] loss:0.09325\n",
      "[1, 10000] loss:0.09323\n",
      "[1, 12000] loss:0.08571\n",
      "epoch: 1 train accuary: 0.9147583333333333  test accuary: 0.98065\n",
      "[2,  2000] loss:0.07635\n",
      "[2,  4000] loss:0.06410\n",
      "[2,  6000] loss:0.06172\n",
      "[2,  8000] loss:0.04728\n",
      "[2, 10000] loss:0.05689\n",
      "[2, 12000] loss:0.05484\n",
      "epoch: 2 train accuary: 0.9813  test accuary: 0.985\n",
      "[3,  2000] loss:0.05041\n",
      "[3,  4000] loss:0.04336\n",
      "[3,  6000] loss:0.04487\n",
      "[3,  8000] loss:0.03434\n",
      "[3, 10000] loss:0.04339\n",
      "[3, 12000] loss:0.04215\n",
      "epoch: 3 train accuary: 0.9869166666666667  test accuary: 0.98675\n",
      "[4,  2000] loss:0.03898\n",
      "[4,  4000] loss:0.03374\n",
      "[4,  6000] loss:0.03673\n",
      "[4,  8000] loss:0.02662\n",
      "[4, 10000] loss:0.03601\n",
      "[4, 12000] loss:0.03400\n",
      "epoch: 4 train accuary: 0.9895583333333333  test accuary: 0.9884\n",
      "[5,  2000] loss:0.02962\n",
      "[5,  4000] loss:0.02704\n",
      "[5,  6000] loss:0.03170\n",
      "[5,  8000] loss:0.02287\n",
      "[5, 10000] loss:0.02972\n",
      "[5, 12000] loss:0.02707\n",
      "epoch: 5 train accuary: 0.99125  test accuary: 0.98855\n",
      "[6,  2000] loss:0.02426\n",
      "[6,  4000] loss:0.02198\n",
      "[6,  6000] loss:0.02699\n",
      "[6,  8000] loss:0.01939\n",
      "[6, 10000] loss:0.02553\n",
      "[6, 12000] loss:0.02284\n",
      "epoch: 6 train accuary: 0.9926833333333334  test accuary: 0.98875\n",
      "[7,  2000] loss:0.02120\n",
      "[7,  4000] loss:0.01811\n",
      "[7,  6000] loss:0.02345\n",
      "[7,  8000] loss:0.01691\n",
      "[7, 10000] loss:0.02140\n",
      "[7, 12000] loss:0.01947\n",
      "epoch: 7 train accuary: 0.993525  test accuary: 0.9888\n",
      "[8,  2000] loss:0.01701\n",
      "[8,  4000] loss:0.01614\n",
      "[8,  6000] loss:0.02079\n",
      "[8,  8000] loss:0.01417\n",
      "[8, 10000] loss:0.01732\n",
      "[8, 12000] loss:0.01570\n",
      "epoch: 8 train accuary: 0.9946583333333333  test accuary: 0.98875\n",
      "[9,  2000] loss:0.01398\n",
      "[9,  4000] loss:0.01242\n",
      "[9,  6000] loss:0.01770\n",
      "[9,  8000] loss:0.01254\n",
      "[9, 10000] loss:0.01568\n",
      "[9, 12000] loss:0.01298\n",
      "epoch: 9 train accuary: 0.9954  test accuary: 0.9897\n",
      "[10,  2000] loss:0.01226\n",
      "[10,  4000] loss:0.01394\n",
      "[10,  6000] loss:0.01602\n",
      "[10,  8000] loss:0.01143\n",
      "[10, 10000] loss:0.01473\n",
      "[10, 12000] loss:0.01239\n",
      "epoch: 10 train accuary: 0.995525  test accuary: 0.98925\n",
      "[11,  2000] loss:0.01115\n",
      "[11,  4000] loss:0.01144\n",
      "[11,  6000] loss:0.01621\n",
      "[11,  8000] loss:0.01009\n",
      "[11, 10000] loss:0.01379\n",
      "[11, 12000] loss:0.01133\n",
      "epoch: 11 train accuary: 0.99565  test accuary: 0.98955\n",
      "[12,  2000] loss:0.01090\n",
      "[12,  4000] loss:0.00872\n",
      "[12,  6000] loss:0.01165\n",
      "[12,  8000] loss:0.01102\n",
      "[12, 10000] loss:0.01147\n",
      "[12, 12000] loss:0.00985\n",
      "epoch: 12 train accuary: 0.9963083333333334  test accuary: 0.99005\n",
      "[13,  2000] loss:0.00987\n",
      "[13,  4000] loss:0.01447\n",
      "[13,  6000] loss:0.01259\n",
      "[13,  8000] loss:0.00804\n",
      "[13, 10000] loss:0.00990\n",
      "[13, 12000] loss:0.01062\n",
      "epoch: 13 train accuary: 0.996325  test accuary: 0.9894\n",
      "[14,  2000] loss:0.00651\n",
      "[14,  4000] loss:0.00908\n",
      "[14,  6000] loss:0.01081\n",
      "[14,  8000] loss:0.00795\n",
      "[14, 10000] loss:0.00911\n",
      "[14, 12000] loss:0.00931\n",
      "epoch: 14 train accuary: 0.9971333333333333  test accuary: 0.98955\n",
      "[15,  2000] loss:0.00575\n",
      "[15,  4000] loss:0.00663\n",
      "[15,  6000] loss:0.00952\n",
      "[15,  8000] loss:0.00681\n",
      "[15, 10000] loss:0.01120\n",
      "[15, 12000] loss:0.00766\n",
      "epoch: 15 train accuary: 0.9973166666666666  test accuary: 0.9895\n",
      "[16,  2000] loss:0.00897\n",
      "[16,  4000] loss:0.00994\n",
      "[16,  6000] loss:0.00946\n",
      "[16,  8000] loss:0.00760\n",
      "[16, 10000] loss:0.00848\n",
      "[16, 12000] loss:0.00732\n",
      "epoch: 16 train accuary: 0.9970583333333334  test accuary: 0.989\n",
      "[17,  2000] loss:0.00767\n",
      "[17,  4000] loss:0.00854\n",
      "[17,  6000] loss:0.00910\n",
      "[17,  8000] loss:0.00410\n",
      "[17, 10000] loss:0.00746\n",
      "[17, 12000] loss:0.00577\n",
      "epoch: 17 train accuary: 0.9975583333333333  test accuary: 0.9883\n",
      "[18,  2000] loss:0.00708\n",
      "[18,  4000] loss:0.00531\n",
      "[18,  6000] loss:0.00696\n",
      "[18,  8000] loss:0.00505\n",
      "[18, 10000] loss:0.00886\n",
      "[18, 12000] loss:0.00471\n",
      "epoch: 18 train accuary: 0.9977583333333333  test accuary: 0.99035\n",
      "[19,  2000] loss:0.00515\n",
      "[19,  4000] loss:0.00543\n",
      "[19,  6000] loss:0.00735\n",
      "[19,  8000] loss:0.00640\n",
      "[19, 10000] loss:0.00499\n",
      "[19, 12000] loss:0.00524\n",
      "epoch: 19 train accuary: 0.9979083333333333  test accuary: 0.9886\n",
      "[20,  2000] loss:0.00413\n",
      "[20,  4000] loss:0.00751\n",
      "[20,  6000] loss:0.00975\n",
      "[20,  8000] loss:0.00782\n",
      "[20, 10000] loss:0.00645\n",
      "[20, 12000] loss:0.00501\n",
      "epoch: 20 train accuary: 0.99765  test accuary: 0.9897\n",
      "[21,  2000] loss:0.00331\n",
      "[21,  4000] loss:0.00920\n",
      "[21,  6000] loss:0.00622\n",
      "[21,  8000] loss:0.00291\n",
      "[21, 10000] loss:0.00439\n",
      "[21, 12000] loss:0.00587\n",
      "epoch: 21 train accuary: 0.9980333333333333  test accuary: 0.98905\n",
      "[22,  2000] loss:0.00324\n",
      "[22,  4000] loss:0.00511\n",
      "[22,  6000] loss:0.00552\n",
      "[22,  8000] loss:0.00414\n",
      "[22, 10000] loss:0.00437\n",
      "[22, 12000] loss:0.00672\n",
      "epoch: 22 train accuary: 0.9983833333333333  test accuary: 0.98925\n",
      "[23,  2000] loss:0.00384\n",
      "[23,  4000] loss:0.00328\n",
      "[23,  6000] loss:0.00383\n",
      "[23,  8000] loss:0.00222\n",
      "[23, 10000] loss:0.00374\n",
      "[23, 12000] loss:0.00299\n",
      "epoch: 23 train accuary: 0.9989666666666667  test accuary: 0.98975\n",
      "[24,  2000] loss:0.00340\n",
      "[24,  4000] loss:0.00699\n",
      "[24,  6000] loss:0.00446\n",
      "[24,  8000] loss:0.00212\n",
      "[24, 10000] loss:0.00344\n",
      "[24, 12000] loss:0.00412\n",
      "epoch: 24 train accuary: 0.998625  test accuary: 0.9907\n",
      "[25,  2000] loss:0.00348\n",
      "[25,  4000] loss:0.00689\n",
      "[25,  6000] loss:0.00451\n",
      "[25,  8000] loss:0.00640\n",
      "[25, 10000] loss:0.00764\n",
      "[25, 12000] loss:0.00830\n",
      "epoch: 25 train accuary: 0.9980583333333334  test accuary: 0.98735\n",
      "[26,  2000] loss:0.00310\n",
      "[26,  4000] loss:0.00513\n",
      "[26,  6000] loss:0.00565\n",
      "[26,  8000] loss:0.00298\n",
      "[26, 10000] loss:0.00481\n",
      "[26, 12000] loss:0.00840\n",
      "epoch: 26 train accuary: 0.9984166666666666  test accuary: 0.98885\n",
      "[27,  2000] loss:0.00809\n",
      "[27,  4000] loss:0.00515\n",
      "[27,  6000] loss:0.00541\n",
      "[27,  8000] loss:0.00260\n",
      "[27, 10000] loss:0.00286\n",
      "[27, 12000] loss:0.00229\n",
      "epoch: 27 train accuary: 0.99855  test accuary: 0.98845\n",
      "[28,  2000] loss:0.00358\n",
      "[28,  4000] loss:0.00244\n",
      "[28,  6000] loss:0.00212\n",
      "[28,  8000] loss:0.00229\n",
      "[28, 10000] loss:0.00268\n",
      "[28, 12000] loss:0.00232\n",
      "epoch: 28 train accuary: 0.9991333333333333  test accuary: 0.99005\n",
      "[29,  2000] loss:0.00237\n",
      "[29,  4000] loss:0.00284\n",
      "[29,  6000] loss:0.00396\n",
      "[29,  8000] loss:0.00516\n",
      "[29, 10000] loss:0.00499\n",
      "[29, 12000] loss:0.00405\n",
      "epoch: 29 train accuary: 0.9986583333333333  test accuary: 0.9883\n",
      "[30,  2000] loss:0.00193\n",
      "[30,  4000] loss:0.00371\n",
      "[30,  6000] loss:0.00616\n",
      "[30,  8000] loss:0.00959\n",
      "[30, 10000] loss:0.00662\n",
      "[30, 12000] loss:0.00451\n",
      "epoch: 30 train accuary: 0.99825  test accuary: 0.989\n",
      "[31,  2000] loss:0.00323\n",
      "[31,  4000] loss:0.00352\n",
      "[31,  6000] loss:0.00292\n",
      "[31,  8000] loss:0.00143\n",
      "[31, 10000] loss:0.00618\n",
      "[31, 12000] loss:0.00373\n",
      "epoch: 31 train accuary: 0.9988583333333333  test accuary: 0.98935\n",
      "[32,  2000] loss:0.00519\n",
      "[32,  4000] loss:0.00310\n",
      "[32,  6000] loss:0.00192\n",
      "[32,  8000] loss:0.00233\n",
      "[32, 10000] loss:0.00239\n",
      "[32, 12000] loss:0.00225\n",
      "epoch: 32 train accuary: 0.99905  test accuary: 0.9904\n",
      "[33,  2000] loss:0.00242\n",
      "[33,  4000] loss:0.00267\n",
      "[33,  6000] loss:0.00352\n",
      "[33,  8000] loss:0.00246\n",
      "[33, 10000] loss:0.00246\n",
      "[33, 12000] loss:0.00199\n",
      "epoch: 33 train accuary: 0.9991  test accuary: 0.99065\n",
      "[34,  2000] loss:0.00446\n",
      "[34,  4000] loss:0.00596\n",
      "[34,  6000] loss:0.00849\n",
      "[34,  8000] loss:0.00530\n",
      "[34, 10000] loss:0.00766\n",
      "[34, 12000] loss:0.01086\n",
      "epoch: 34 train accuary: 0.9977  test accuary: 0.9888\n",
      "[35,  2000] loss:0.00390\n",
      "[35,  4000] loss:0.00501\n",
      "[35,  6000] loss:0.00460\n",
      "[35,  8000] loss:0.00439\n",
      "[35, 10000] loss:0.00408\n",
      "[35, 12000] loss:0.00340\n",
      "epoch: 35 train accuary: 0.9984  test accuary: 0.99075\n",
      "[36,  2000] loss:0.00226\n",
      "[36,  4000] loss:0.00359\n",
      "[36,  6000] loss:0.00409\n",
      "[36,  8000] loss:0.00211\n",
      "[36, 10000] loss:0.00164\n",
      "[36, 12000] loss:0.00111\n",
      "epoch: 36 train accuary: 0.999075  test accuary: 0.99\n",
      "[37,  2000] loss:0.00294\n",
      "[37,  4000] loss:0.00189\n",
      "[37,  6000] loss:0.00276\n",
      "[37,  8000] loss:0.00129\n",
      "[37, 10000] loss:0.00138\n",
      "[37, 12000] loss:0.00169\n",
      "epoch: 37 train accuary: 0.9993666666666666  test accuary: 0.9898\n",
      "[38,  2000] loss:0.00154\n",
      "[38,  4000] loss:0.00229\n",
      "[38,  6000] loss:0.00129\n",
      "[38,  8000] loss:0.00095\n",
      "[38, 10000] loss:0.00149\n",
      "[38, 12000] loss:0.00156\n",
      "epoch: 38 train accuary: 0.9995333333333334  test accuary: 0.9897\n",
      "[39,  2000] loss:0.00196\n",
      "[39,  4000] loss:0.00177\n",
      "[39,  6000] loss:0.00235\n",
      "[39,  8000] loss:0.00198\n",
      "[39, 10000] loss:0.00176\n",
      "[39, 12000] loss:0.00211\n",
      "epoch: 39 train accuary: 0.9993583333333333  test accuary: 0.99005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,  2000] loss:0.00113\n",
      "[40,  4000] loss:0.00102\n",
      "[40,  6000] loss:0.00110\n",
      "[40,  8000] loss:0.00035\n",
      "[40, 10000] loss:0.00031\n",
      "[40, 12000] loss:0.00060\n",
      "epoch: 40 train accuary: 0.99975  test accuary: 0.9907\n",
      "-----Training ends-----\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:1 total time used: 18.262924393018086\n",
      "Best train accuracy among 5 runs: 99.97500000000001\n",
      "Best test accuracy among 5 runs: 99.07000000000001\n"
     ]
    }
   ],
   "source": [
    "channel=1\n",
    "size=28\n",
    "epoch=40\n",
    "learning_rate=0.001\n",
    "momentum=0.9\n",
    "weight_decay=1e-5\n",
    "meanTrainResults(device,trainData,testData,channel,size,epoch,learning_rate,momentum,weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss:1.10426\n",
      "[1,  4000] loss:0.17823\n",
      "[1,  6000] loss:0.12495\n",
      "[1,  8000] loss:0.10053\n",
      "[1, 10000] loss:0.09272\n",
      "[1, 12000] loss:0.08789\n",
      "epoch: 1 train accuary: 0.9071833333333333  test accuary: 0.9809\n",
      "[2,  2000] loss:0.07933\n",
      "[2,  4000] loss:0.06760\n",
      "[2,  6000] loss:0.06362\n",
      "[2,  8000] loss:0.05068\n",
      "[2, 10000] loss:0.05992\n",
      "[2, 12000] loss:0.05657\n",
      "epoch: 2 train accuary: 0.9801916666666667  test accuary: 0.9835\n",
      "[3,  2000] loss:0.05408\n",
      "[3,  4000] loss:0.04724\n",
      "[3,  6000] loss:0.04555\n",
      "[3,  8000] loss:0.03509\n",
      "[3, 10000] loss:0.04536\n",
      "[3, 12000] loss:0.04233\n",
      "epoch: 3 train accuary: 0.986025  test accuary: 0.98525\n",
      "[4,  2000] loss:0.03934\n",
      "[4,  4000] loss:0.03518\n",
      "[4,  6000] loss:0.03669\n",
      "[4,  8000] loss:0.02736\n",
      "[4, 10000] loss:0.03757\n",
      "[4, 12000] loss:0.03544\n",
      "epoch: 4 train accuary: 0.988675  test accuary: 0.9853\n",
      "[5,  2000] loss:0.03050\n",
      "[5,  4000] loss:0.02854\n",
      "[5,  6000] loss:0.03104\n",
      "[5,  8000] loss:0.02292\n",
      "[5, 10000] loss:0.03114\n",
      "[5, 12000] loss:0.03019\n",
      "epoch: 5 train accuary: 0.99095  test accuary: 0.98855\n",
      "[6,  2000] loss:0.02430\n",
      "[6,  4000] loss:0.02288\n",
      "[6,  6000] loss:0.02760\n",
      "[6,  8000] loss:0.01895\n",
      "[6, 10000] loss:0.02467\n",
      "[6, 12000] loss:0.02562\n",
      "epoch: 6 train accuary: 0.9925333333333334  test accuary: 0.988\n",
      "[7,  2000] loss:0.02091\n",
      "[7,  4000] loss:0.01882\n",
      "[7,  6000] loss:0.02185\n",
      "[7,  8000] loss:0.01585\n",
      "[7, 10000] loss:0.02167\n",
      "[7, 12000] loss:0.02157\n",
      "epoch: 7 train accuary: 0.9939583333333334  test accuary: 0.9883\n",
      "[8,  2000] loss:0.01726\n",
      "[8,  4000] loss:0.01439\n",
      "[8,  6000] loss:0.01949\n",
      "[8,  8000] loss:0.01334\n",
      "[8, 10000] loss:0.01917\n",
      "[8, 12000] loss:0.01875\n",
      "epoch: 8 train accuary: 0.9947666666666667  test accuary: 0.9893\n",
      "[9,  2000] loss:0.01249\n",
      "[9,  4000] loss:0.01294\n",
      "[9,  6000] loss:0.01858\n",
      "[9,  8000] loss:0.01197\n",
      "[9, 10000] loss:0.01637\n",
      "[9, 12000] loss:0.01530\n",
      "epoch: 9 train accuary: 0.9954166666666666  test accuary: 0.99075\n",
      "[10,  2000] loss:0.01256\n",
      "[10,  4000] loss:0.01039\n",
      "[10,  6000] loss:0.01606\n",
      "[10,  8000] loss:0.01085\n",
      "[10, 10000] loss:0.01384\n",
      "[10, 12000] loss:0.01363\n",
      "epoch: 10 train accuary: 0.995975  test accuary: 0.98885\n",
      "[11,  2000] loss:0.00982\n",
      "[11,  4000] loss:0.00941\n",
      "[11,  6000] loss:0.01355\n",
      "[11,  8000] loss:0.01048\n",
      "[11, 10000] loss:0.01242\n",
      "[11, 12000] loss:0.01121\n",
      "epoch: 11 train accuary: 0.9965416666666667  test accuary: 0.9909\n",
      "[12,  2000] loss:0.00853\n",
      "[12,  4000] loss:0.00745\n",
      "[12,  6000] loss:0.01109\n",
      "[12,  8000] loss:0.00935\n",
      "[12, 10000] loss:0.01173\n",
      "[12, 12000] loss:0.01112\n",
      "epoch: 12 train accuary: 0.9969833333333333  test accuary: 0.9911\n",
      "[13,  2000] loss:0.00821\n",
      "[13,  4000] loss:0.00852\n",
      "[13,  6000] loss:0.01219\n",
      "[13,  8000] loss:0.00760\n",
      "[13, 10000] loss:0.00987\n",
      "[13, 12000] loss:0.00819\n",
      "epoch: 13 train accuary: 0.997225  test accuary: 0.99095\n",
      "[14,  2000] loss:0.00780\n",
      "[14,  4000] loss:0.01394\n",
      "[14,  6000] loss:0.01089\n",
      "[14,  8000] loss:0.00732\n",
      "[14, 10000] loss:0.01052\n",
      "[14, 12000] loss:0.00929\n",
      "epoch: 14 train accuary: 0.9968083333333333  test accuary: 0.9891\n",
      "[15,  2000] loss:0.00676\n",
      "[15,  4000] loss:0.00745\n",
      "[15,  6000] loss:0.00946\n",
      "[15,  8000] loss:0.00647\n",
      "[15, 10000] loss:0.01122\n",
      "[15, 12000] loss:0.00918\n",
      "epoch: 15 train accuary: 0.99735  test accuary: 0.99055\n",
      "[16,  2000] loss:0.00543\n",
      "[16,  4000] loss:0.00643\n",
      "[16,  6000] loss:0.00706\n",
      "[16,  8000] loss:0.00457\n",
      "[16, 10000] loss:0.00675\n",
      "[16, 12000] loss:0.00598\n",
      "epoch: 16 train accuary: 0.9981333333333333  test accuary: 0.9904\n",
      "[17,  2000] loss:0.00380\n",
      "[17,  4000] loss:0.00522\n",
      "[17,  6000] loss:0.00829\n",
      "[17,  8000] loss:0.00558\n",
      "[17, 10000] loss:0.00794\n",
      "[17, 12000] loss:0.00784\n",
      "epoch: 17 train accuary: 0.99795  test accuary: 0.99095\n",
      "[18,  2000] loss:0.01577\n",
      "[18,  4000] loss:0.00815\n",
      "[18,  6000] loss:0.00865\n",
      "[18,  8000] loss:0.00548\n",
      "[18, 10000] loss:0.00673\n",
      "[18, 12000] loss:0.00465\n",
      "epoch: 18 train accuary: 0.9972333333333333  test accuary: 0.9908\n",
      "[19,  2000] loss:0.00580\n",
      "[19,  4000] loss:0.00391\n",
      "[19,  6000] loss:0.00483\n",
      "[19,  8000] loss:0.00658\n",
      "[19, 10000] loss:0.00777\n",
      "[19, 12000] loss:0.00649\n",
      "epoch: 19 train accuary: 0.9982666666666666  test accuary: 0.991\n",
      "[20,  2000] loss:0.00518\n",
      "[20,  4000] loss:0.00447\n",
      "[20,  6000] loss:0.00566\n",
      "[20,  8000] loss:0.00320\n",
      "[20, 10000] loss:0.00574\n",
      "[20, 12000] loss:0.00432\n",
      "epoch: 20 train accuary: 0.998575  test accuary: 0.9901\n",
      "[21,  2000] loss:0.00395\n",
      "[21,  4000] loss:0.00254\n",
      "[21,  6000] loss:0.00383\n",
      "[21,  8000] loss:0.00281\n",
      "[21, 10000] loss:0.00595\n",
      "[21, 12000] loss:0.00513\n",
      "epoch: 21 train accuary: 0.9988083333333333  test accuary: 0.9913\n",
      "[22,  2000] loss:0.00203\n",
      "[22,  4000] loss:0.00303\n",
      "[22,  6000] loss:0.00333\n",
      "[22,  8000] loss:0.00223\n",
      "[22, 10000] loss:0.00522\n",
      "[22, 12000] loss:0.00448\n",
      "epoch: 22 train accuary: 0.998975  test accuary: 0.9911\n",
      "[23,  2000] loss:0.00393\n",
      "[23,  4000] loss:0.00375\n",
      "[23,  6000] loss:0.00401\n",
      "[23,  8000] loss:0.00292\n",
      "[23, 10000] loss:0.00551\n",
      "[23, 12000] loss:0.00277\n",
      "epoch: 23 train accuary: 0.9988916666666666  test accuary: 0.99035\n",
      "[24,  2000] loss:0.00209\n",
      "[24,  4000] loss:0.00285\n",
      "[24,  6000] loss:0.00254\n",
      "[24,  8000] loss:0.00133\n",
      "[24, 10000] loss:0.00322\n",
      "[24, 12000] loss:0.00160\n",
      "epoch: 24 train accuary: 0.9994583333333333  test accuary: 0.99125\n",
      "[25,  2000] loss:0.00165\n",
      "[25,  4000] loss:0.00404\n",
      "[25,  6000] loss:0.00253\n",
      "[25,  8000] loss:0.00188\n",
      "[25, 10000] loss:0.00311\n",
      "[25, 12000] loss:0.00643\n",
      "epoch: 25 train accuary: 0.998925  test accuary: 0.9914\n",
      "[26,  2000] loss:0.00402\n",
      "[26,  4000] loss:0.00331\n",
      "[26,  6000] loss:0.00338\n",
      "[26,  8000] loss:0.00404\n",
      "[26, 10000] loss:0.00617\n",
      "[26, 12000] loss:0.00392\n",
      "epoch: 26 train accuary: 0.9986916666666666  test accuary: 0.9903\n",
      "[27,  2000] loss:0.00254\n",
      "[27,  4000] loss:0.00185\n",
      "[27,  6000] loss:0.00283\n",
      "[27,  8000] loss:0.00161\n",
      "[27, 10000] loss:0.00459\n",
      "[27, 12000] loss:0.00162\n",
      "epoch: 27 train accuary: 0.9993333333333333  test accuary: 0.99075\n",
      "[28,  2000] loss:0.00160\n",
      "[28,  4000] loss:0.00152\n",
      "[28,  6000] loss:0.00125\n",
      "[28,  8000] loss:0.00064\n",
      "[28, 10000] loss:0.00187\n",
      "[28, 12000] loss:0.00190\n",
      "epoch: 28 train accuary: 0.9996333333333334  test accuary: 0.9909\n",
      "[29,  2000] loss:0.00085\n",
      "[29,  4000] loss:0.00076\n",
      "[29,  6000] loss:0.00090\n",
      "[29,  8000] loss:0.00063\n",
      "[29, 10000] loss:0.00362\n",
      "[29, 12000] loss:0.00129\n",
      "epoch: 29 train accuary: 0.9997333333333334  test accuary: 0.9919\n",
      "[30,  2000] loss:0.00051\n",
      "[30,  4000] loss:0.00066\n",
      "[30,  6000] loss:0.00103\n",
      "[30,  8000] loss:0.00064\n",
      "[30, 10000] loss:0.00112\n",
      "[30, 12000] loss:0.00068\n",
      "epoch: 30 train accuary: 0.9999083333333333  test accuary: 0.9919\n",
      "[31,  2000] loss:0.00037\n",
      "[31,  4000] loss:0.00048\n",
      "[31,  6000] loss:0.00048\n",
      "[31,  8000] loss:0.00041\n",
      "[31, 10000] loss:0.00072\n",
      "[31, 12000] loss:0.00047\n",
      "epoch: 31 train accuary: 0.999975  test accuary: 0.992\n",
      "[32,  2000] loss:0.00033\n",
      "[32,  4000] loss:0.00038\n",
      "[32,  6000] loss:0.00044\n",
      "[32,  8000] loss:0.00031\n",
      "[32, 10000] loss:0.00063\n",
      "[32, 12000] loss:0.00049\n",
      "epoch: 32 train accuary: 0.9999583333333333  test accuary: 0.99205\n",
      "[33,  2000] loss:0.00029\n",
      "[33,  4000] loss:0.00032\n",
      "[33,  6000] loss:0.00042\n",
      "[33,  8000] loss:0.00029\n",
      "[33, 10000] loss:0.00054\n",
      "[33, 12000] loss:0.00034\n",
      "epoch: 33 train accuary: 0.999975  test accuary: 0.9919\n",
      "[34,  2000] loss:0.00026\n",
      "[34,  4000] loss:0.00032\n",
      "[34,  6000] loss:0.00037\n",
      "[34,  8000] loss:0.00028\n",
      "[34, 10000] loss:0.00050\n",
      "[34, 12000] loss:0.00034\n",
      "epoch: 34 train accuary: 0.9999916666666666  test accuary: 0.992\n",
      "[35,  2000] loss:0.00025\n",
      "[35,  4000] loss:0.00031\n",
      "[35,  6000] loss:0.00038\n",
      "[35,  8000] loss:0.00027\n",
      "[35, 10000] loss:0.00045\n",
      "[35, 12000] loss:0.00031\n",
      "epoch: 35 train accuary: 0.9999916666666666  test accuary: 0.992\n",
      "[36,  2000] loss:0.00026\n",
      "[36,  4000] loss:0.00030\n",
      "[36,  6000] loss:0.00036\n",
      "[36,  8000] loss:0.00027\n",
      "[36, 10000] loss:0.00043\n",
      "[36, 12000] loss:0.00033\n",
      "epoch: 36 train accuary: 0.9999916666666666  test accuary: 0.992\n",
      "[37,  2000] loss:0.00027\n",
      "[37,  4000] loss:0.00031\n",
      "[37,  6000] loss:0.00037\n",
      "[37,  8000] loss:0.00028\n",
      "[37, 10000] loss:0.00042\n",
      "[37, 12000] loss:0.00032\n",
      "epoch: 37 train accuary: 1.0  test accuary: 0.9921\n",
      "[38,  2000] loss:0.00027\n",
      "[38,  4000] loss:0.00031\n",
      "[38,  6000] loss:0.00038\n",
      "[38,  8000] loss:0.00028\n",
      "[38, 10000] loss:0.00042\n",
      "[38, 12000] loss:0.00034\n",
      "epoch: 38 train accuary: 1.0  test accuary: 0.992\n",
      "[39,  2000] loss:0.00028\n",
      "[39,  4000] loss:0.00032\n",
      "[39,  6000] loss:0.00038\n",
      "[39,  8000] loss:0.00029\n",
      "[39, 10000] loss:0.00043\n",
      "[39, 12000] loss:0.00034\n",
      "epoch: 39 train accuary: 1.0  test accuary: 0.99215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,  2000] loss:0.00029\n",
      "[40,  4000] loss:0.00033\n",
      "[40,  6000] loss:0.00039\n",
      "[40,  8000] loss:0.00030\n",
      "[40, 10000] loss:0.00044\n",
      "[40, 12000] loss:0.00036\n",
      "epoch: 40 train accuary: 1.0  test accuary: 0.9923\n",
      "[41,  2000] loss:0.00030\n",
      "[41,  4000] loss:0.00033\n",
      "[41,  6000] loss:0.00040\n",
      "[41,  8000] loss:0.00031\n",
      "[41, 10000] loss:0.00045\n",
      "[41, 12000] loss:0.00036\n",
      "epoch: 41 train accuary: 1.0  test accuary: 0.99215\n",
      "[42,  2000] loss:0.00031\n",
      "[42,  4000] loss:0.00034\n",
      "[42,  6000] loss:0.00041\n",
      "[42,  8000] loss:0.00031\n",
      "[42, 10000] loss:0.00045\n",
      "[42, 12000] loss:0.00037\n",
      "epoch: 42 train accuary: 1.0  test accuary: 0.99225\n",
      "[43,  2000] loss:0.00032\n",
      "[43,  4000] loss:0.00035\n",
      "[43,  6000] loss:0.00042\n",
      "[43,  8000] loss:0.00032\n",
      "[43, 10000] loss:0.00047\n",
      "[43, 12000] loss:0.00039\n",
      "epoch: 43 train accuary: 1.0  test accuary: 0.99225\n",
      "[44,  2000] loss:0.00032\n",
      "[44,  4000] loss:0.00036\n",
      "[44,  6000] loss:0.00043\n",
      "[44,  8000] loss:0.00033\n",
      "[44, 10000] loss:0.00045\n",
      "[44, 12000] loss:0.00040\n",
      "epoch: 44 train accuary: 1.0  test accuary: 0.9924\n",
      "[45,  2000] loss:0.00034\n",
      "[45,  4000] loss:0.00036\n",
      "[45,  6000] loss:0.00044\n",
      "[45,  8000] loss:0.00034\n",
      "[45, 10000] loss:0.00047\n",
      "[45, 12000] loss:0.00040\n",
      "epoch: 45 train accuary: 1.0  test accuary: 0.9923\n",
      "[46,  2000] loss:0.00034\n",
      "[46,  4000] loss:0.00037\n",
      "[46,  6000] loss:0.00044\n",
      "[46,  8000] loss:0.00034\n",
      "[46, 10000] loss:0.00048\n",
      "[46, 12000] loss:0.00041\n",
      "epoch: 46 train accuary: 1.0  test accuary: 0.99245\n",
      "[47,  2000] loss:0.00035\n",
      "[47,  4000] loss:0.00038\n",
      "[47,  6000] loss:0.00046\n",
      "[47,  8000] loss:0.00035\n",
      "[47, 10000] loss:0.00048\n",
      "[47, 12000] loss:0.00042\n",
      "epoch: 47 train accuary: 1.0  test accuary: 0.9924\n",
      "[48,  2000] loss:0.00035\n",
      "[48,  4000] loss:0.00038\n",
      "[48,  6000] loss:0.00045\n",
      "[48,  8000] loss:0.00035\n",
      "[48, 10000] loss:0.00048\n",
      "[48, 12000] loss:0.00043\n",
      "epoch: 48 train accuary: 1.0  test accuary: 0.9923\n",
      "[49,  2000] loss:0.00036\n",
      "[49,  4000] loss:0.00039\n",
      "[49,  6000] loss:0.00046\n",
      "[49,  8000] loss:0.00036\n",
      "[49, 10000] loss:0.00049\n",
      "[49, 12000] loss:0.00043\n",
      "epoch: 49 train accuary: 1.0  test accuary: 0.99235\n",
      "[50,  2000] loss:0.00036\n",
      "[50,  4000] loss:0.00039\n",
      "[50,  6000] loss:0.00046\n",
      "[50,  8000] loss:0.00036\n",
      "[50, 10000] loss:0.00049\n",
      "[50, 12000] loss:0.00045\n",
      "epoch: 50 train accuary: 1.0  test accuary: 0.99225\n",
      "-----Training ends-----\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:1 total time used: 22.954620643456778\n",
      "Best train accuracy among 5 runs: 100.0\n",
      "Best test accuracy among 5 runs: 99.225\n"
     ]
    }
   ],
   "source": [
    "channel=1\n",
    "size=28\n",
    "epoch=50\n",
    "learning_rate=0.001\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "meanTrainResults(device,trainData,testData,channel,size,epoch,learning_rate,momentum,weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
